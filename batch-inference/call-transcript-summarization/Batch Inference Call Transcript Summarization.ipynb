{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb6374e",
   "metadata": {},
   "source": [
    "# Batch inference to summarize call transcripts\n",
    "\n",
    "## Introduction\n",
    "Call center transcript summarization is a crucial task for businesses seeking to extract valuable insights from customer interactions. As the volume of call data grows, traditional analysis methods struggle to keep pace, creating a demand for scalable solutions. Batch Inference for Amazon Bedrock provides a powerful tool to address this challenge by enabling organizations to process large volumes of data efficiently.\n",
    "\n",
    "This notebook demonstrates how to leverage batch inference for summarizing call center transcripts at scale. By processing substantial volumes of text transcripts in batches. Though we are using example of call transcript summarization here, you can really apply this to any other use case that does not need a real time output.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure that you have the following prerequisites in place:\n",
    "1. Updated boto3 to 1.35.1 or greater version\n",
    "2. Have raw data stored in S3 bucket as \n",
    "3. Permissions to invoke `create_model_invocation_job` API. Refer to the documentation to learn about [required permissions for batch inference job](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-permissions.html).\n",
    "4. Permission to read and write data on Amazon S3 bucket.\n",
    "5. Call transcript dataset:\n",
    "* This notebook was built using synthetic call transcripts in `.txt` files. If you want to try it with your own dataset, upload your call transcripts to an Amazon S3 bucket in `.txt` format. Each text file in the S3 bucket should contain only one call transcript.\n",
    "* If you do not have a dataset but want to try out Batch Inference for Amazon Bedrock, you can use the synthetic call data available [here](https://github.com/aws-samples/amazon-bedrock-samples/batch-inference/call-transcript-summarization/synthetic_call_transcript_data.zip). You need to unzip the data, and then upload the `.txt` files to an S3 bucket to use the notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f646a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip\n",
    "%pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Bedrock client for batch inference job\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# # Set the S3 bucket name and prefix for the text files\n",
    "bucket_name = '<your-s3-bucket-name>'\n",
    "raw_data_prefix = '<raw-data-prefix>' + '/'\n",
    "out_prefix = '<prefix-for-batch-output>'\n",
    "\n",
    "# Batch API parameters:\n",
    "role_Arn = \"<ARN of the role with permissions to invoke batch api for amazon bedrock>\"\n",
    "model_input_summary_prefix = '<input-prefix>'\n",
    "jobName = '<name-of-batch-job>'\n",
    "model_id = 'anthropic.claude-3-haiku-20240307-v1:0' # or use other model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5ba61",
   "metadata": {},
   "source": [
    "# Prepare data for the batch inference:\n",
    "## Data Preparation\n",
    "\n",
    "Before initiating a batch inference job for call center transcript summarization, it's crucial to properly format and upload your data to an S3 bucket. Learn more about data format requirments in our [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data.html).\n",
    "\n",
    "### Formatting Input Data\n",
    "\n",
    "The input data should be in JSONL format, with each line representing a single transcript for summarization. Each line in your JSONL file should follow this structure:\n",
    "\n",
    "```json\n",
    "{\"recordId\": \"11 character alphanumeric string\", \"modelInput\": {JSON body}}\n",
    "```\n",
    "\n",
    "Here, `recordId` is an 11-character alphanumeric string, working as a unique identifier for each entry. If you omit this field, the batch inference job will automatically add it in the output.\n",
    "\n",
    "The format of the `modelInput` JSON object should match the body field for the model you are using in the `InvokeModel` request. For example, if you're using the Anthropic Claude 3 model on Amazon Bedrock, you should use the MessageAPI, and your model input might look like the following:\n",
    "\n",
    "```json\n",
    "{\"recordId\": \"CALL0000001\", \n",
    " \"modelInput\": {\n",
    "     \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "     \"max_tokens\": 1024,\n",
    "     \"messages\": [ { \n",
    "           \"role\": \"user\", \n",
    "           \"content\": [{\"type\":\"text\", \"text\":\"{<your-prompt-for-summarization>}: {<your-transcript>}\" }] }],\n",
    "      }\n",
    "}\n",
    "```\n",
    "\n",
    "### Generating Model Inputs\n",
    "\n",
    "The `prepare_model_inputs` function reads the input text files from an Amazon S3 bucket, generates unique record IDs, and prepares the model inputs according to the Anthropic Claude 3 model format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_inputs(bucket_name, prefix):\n",
    "    # Initialize the model_inputs list\n",
    "    model_inputs = []\n",
    "\n",
    "    # List all text files in the S3 bucket\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    # Process each text file\n",
    "    for obj in response.get('Contents', []):\n",
    "        # Get the file path\n",
    "        file_path = obj['Key']\n",
    "\n",
    "        # Read the file content\n",
    "        file_obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "        file_content = file_obj['Body'].read().decode('utf-8')\n",
    "\n",
    "        # Generate a unique record ID\n",
    "        record_id = str(int(datetime.now().timestamp())) # you can replace this with your own logic\n",
    "\n",
    "        # Prepare the input text for the Anthropic API\n",
    "        input_text = f\"\"\"Write an accurate gender-neutral summary of the following text without adding preamble or \n",
    "                        additonal information thats not present in the original text: {file_content}\"\"\"\n",
    "\n",
    "        # Define the request body for the Anthropic API\n",
    "        body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"messages\": [{\"role\": 'user',\n",
    "                           \"content\": [\n",
    "                               {'type': 'text',\n",
    "                                'text': input_text}]\n",
    "                           }],\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.1,\n",
    "            \"top_k\": 100,\n",
    "        }\n",
    "\n",
    "        # Prepare the model input\n",
    "        model_input = {\n",
    "            \"recordId\": record_id,\n",
    "            \"modelInput\": body\n",
    "        }\n",
    "\n",
    "        # Append the model input to the list\n",
    "        model_inputs.append(model_input)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# prepare data for batch inference:\n",
    "model_input_jsonl = prepare_model_inputs(bucket_name, raw_data_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913ba43",
   "metadata": {},
   "source": [
    "### Writing to JSONL File\n",
    "\n",
    "The `write_jsonl` function takes a list of data (in this case, the list of model inputs) and a file path, and writes the data to a local JSONL file.\n",
    "\n",
    "For each item in the data list, the function converts the item to a JSON string using `json.dumps` and writes it to the file, followed by a newline character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data:\n",
    "            json_str = json.dumps(item)\n",
    "            file.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model inputs to a jsonl file\n",
    "filename = 'batch-' + str(int(datetime.now().timestamp())) + '.jsonl'\n",
    "write_jsonl(model_input_jsonl, f'{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a90686",
   "metadata": {},
   "source": [
    "### Uploading to Amazon S3\n",
    "\n",
    "The `upload_to_s3` function uploads a file or directory to an AWS S3 bucket. It takes three arguments:\n",
    "\n",
    "1. `path`: The path to the file or directory to be uploaded.\n",
    "2. `bucket_name`: The name of the S3 bucket.\n",
    "3. `bucket_subfolder` (optional): The name of the subfolder within the S3 bucket where the prepared data should be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(path, bucket_name, bucket_subfolder=None):\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(path):\n",
    "        # If the path is a file, upload it directly\n",
    "        object_name = os.path.basename(path) if bucket_subfolder is None else f\"{bucket_subfolder}/{os.path.basename(path)}\"\n",
    "        try:\n",
    "            s3.upload_file(path, bucket_name, object_name)\n",
    "            print(f\"Successfully uploaded {path} to {bucket_name}/{object_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {path} to S3: {e}\")\n",
    "            return False\n",
    "    elif os.path.isdir(path):\n",
    "        # If the path is a directory, recursively upload all files within it\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, path)\n",
    "                object_name = relative_path if bucket_subfolder is None else f\"{bucket_subfolder}/{relative_path}\"\n",
    "                try:\n",
    "                    s3.upload_file(file_path, bucket_name, object_name)\n",
    "                    print(f\"Successfully uploaded {file_path} to {bucket_name}/{object_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {file_path} to S3: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"{path} is not a file or directory.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b688b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploads the data from local to S3 bucket for batch inference\n",
    "upload_to_s3(path=f\"/home/ec2-user/SageMaker/{filename}\", \n",
    "             bucket_name=bucket_name, \n",
    "             bucket_subfolder=model_input_summary_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586aacfb",
   "metadata": {},
   "source": [
    "## Creating the Batch Inference Job\n",
    "\n",
    "Once the data is prepared and uploaded to an Amazon S3, you can create the batch inference job.\n",
    "\n",
    "### Configuring Input and Output Data\n",
    "\n",
    "Before submitting the batch inference job, you need to configure the input and output data locations in Amazon S3. This is done using the `inputDataConfig` and `outputDataConfig` parameters.\n",
    "\n",
    "The `inputDataConfig` specifies the Amazon S3 URI where the prepared input data (JSONL file) is stored and, the `outputDataConfig` specifies the Amazon S3 URI where the processed output data will be stored by the batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a50314",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket_name}/{model_input_summary_prefix}/{filename}\"\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket_name}/{model_input_summary_prefix}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig, outputDataConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06129a4a",
   "metadata": {},
   "source": [
    "### Submitting the Batch Inference Job\n",
    "\n",
    "To submit the batch inference job, you use the `create_model_invocation_job` API from the Amazon Bedrock client. This API requires the following parameters:\n",
    "\n",
    "- `roleArn`: The Amazon Resource Name (ARN) of the IAM role with permissions to invoke the batch inference API for Amazon Bedrock.\n",
    "- `modelId`: The ID of the model you want to use for batch inference (e.g., `anthropic.claude-3-haiku-20240307-v1:0`).\n",
    "- `jobName`: A name for your batch inference job.\n",
    "- `inputDataConfig`: The configuration for the input data, as defined in the previous step.\n",
    "- `outputDataConfig`: The configuration for the output data, as defined in the previous step.\n",
    "\n",
    "The API call returns a response containing the ARN of the submitted batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=bedrock.create_model_invocation_job(\n",
    "    roleArn=roleArn,\n",
    "    modelId=model_id,\n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d3b15",
   "metadata": {},
   "source": [
    "### Monitoring Job Status\n",
    "\n",
    "After submitting the batch inference job, you can monitor its status using the `get_model_invocation_job` API from the Amazon Bedrock client. This API requires the `jobIdentifier` parameter, which is the ARN of the submitted job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ed4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobArn)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62690e19",
   "metadata": {},
   "source": [
    "## Retrieving and Analyzing Output\n",
    "\n",
    "When your batch inference job is complete, Amazon Bedrock creates a dedicated folder in the specified S3 bucket, using the job ID as the folder name. This folder contains a summary of the batch inference job, along with the processed inference data in JSONL format.\n",
    "\n",
    "### Accessing and Understanding Output Format\n",
    "\n",
    "The output files contain the processed text, observability data, and the parameters used for inference. The format of the output data will depend on the model you used for batch inference. The notebook provides an example of how to access and process this information from the output JSONL file for Anthropic Claude 3 models.\n",
    "\n",
    "Additionally, in the output location specified for your batch inference job, you'll find a `manifest.json.out` file that provides a summary of the processed records. This file includes information such as the total number of records processed, the number of successfully processed records, the number of records with errors, and the total input and output token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19072d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the S3 bucket name and prefix for the text files. \n",
    "# Last part in the path is the batch job's job id\n",
    "prefix = f\"{model_input_summary_prefix}/{output_prefix}/{job_id}/\"\n",
    "\n",
    "# Initialize the list\n",
    "output_data = []\n",
    "\n",
    "# Read the JSON file from S3\n",
    "try:\n",
    "    object_key = f\"{prefix}{filename}.out\"\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    json_data = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Process the JSON data\n",
    "    for line in json_data.splitlines():\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        output_entry = {\n",
    "            'request_id': data['recordId'],\n",
    "            'output_text': data['modelOutput']['content'][0]['text'],\n",
    "            'observability': {\n",
    "                'input_tokens': data['modelOutput']['usage']['input_tokens'],\n",
    "                'output_tokens': data['modelOutput']['usage']['output_tokens'],\n",
    "                'model': data['modelOutput']['model'],\n",
    "                'stop_reason': data['modelOutput']['stop_reason'],\n",
    "                'request_id': data['recordId'],\n",
    "                'max_tokens': data['modelInput']['max_tokens'],\n",
    "                'temperature': data['modelInput']['temperature'],\n",
    "                'top_p': data['modelInput']['top_p'],\n",
    "                'top_k': data['modelInput']['top_k']\n",
    "            }\n",
    "        }\n",
    "        output_data.append(output_entry)\n",
    "    print(f\"Successfully read {len(output_data)} JSON objects from S3.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file from S3: {e}\")\n",
    "    \n",
    "print(\"sample output:\")\n",
    "print(output_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f16b7",
   "metadata": {},
   "source": [
    "### Integrating with Existing Workflows\n",
    "\n",
    "After retrieving the processed output data, you can integrate it into your existing workflows or analytics systems for further analysis or downstream processing. For example, you could:\n",
    "\n",
    "- Store the summarized transcripts in a database for easy access and querying.\n",
    "- Perform sentiment analysis or topic modeling on the summarized transcripts to gain additional insights.\n",
    "- Categorize the summarizes into actionable business buckets and develop anomaly detection.\n",
    "- Develop dashboards or reports to visualize and analyze the summarized data.\n",
    "\n",
    "The specific integration steps will depend on your existing workflows and systems, but the processed output data from the batch inference job can be easily incorporated into various data pipelines and analytics processes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The notebook covers the entire process, from data preparation and formatting to job submission, output retrieval, and integration with existing workflows. By implementing batch inference for call transcript summarization, you can streamline your analysis processes and gain a competitive edge in understanding customer needs and improving your call center operations.\n",
    "\n",
    "Feel free to adapt and extend this notebook to suit your specific requirements, and explore other use cases where batch inference can be applied to optimize your interactions with foundation models at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

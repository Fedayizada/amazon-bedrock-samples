{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Cross-region Inference in Amazon Bedrock\n",
    "Inference Profiles via Amazon Bedrock allow you to perform cross-region inference without the need to setup anything in a fully-managed, secure and private manner. Inference profiles is a powerful feature built on Amazon Bedrock, offering generative AI application builders a seamless solution for managing traffic bursts and ensuring optimal availability and performance. By leveraging this feature, builders no longer have to spend time and effort building complex resiliency structure. Instead, inference profiles intelligently routes traffic across multiple opted-in regions, automatically adapting to peak utilization surges. Any inferences carried out through inference profiles will dynamically utilize on-demand capacity available from any of the configured regions, maximizing availability and throughput.\n",
    "\n",
    "### Key Features & Benefits:\n",
    "Some of the key features include:\n",
    "\n",
    "* Access to capacity in different regions allowing generative AI workloads to scale with demand.\n",
    "* Access to region-agnostic models to achieve higher throughput.\n",
    "* Become resilient to any traffic bursts.\n",
    "* Ability to select between the pre-defined region sets suiting application needs.\n",
    "* Compatible with existing APIs.\n",
    "* No additional cost.\n",
    "* Same model pricing as the primary region.\n",
    "\n",
    "The end-user experience is not impacted as the model under inference profiles remain the same, and builder can focus on writing logic for a differentiated application. \n",
    "\n",
    "Bedrock is the easiest way to build and scale generative AI applications. Cross region inference further enhances the usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by installing the dependencies to ensure we have a recent version\n",
    "!pip install --quiet --upgrade --force-reinstall boto3 botocore awscli\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ IMPORTANT‚ùóüî¥\n",
    "\n",
    "*Note:* <span style=\"color:red\">This notebook sample uses `us-east-1` region and the inference profiles available there as example.\n",
    "Change the `region_name` and inference profile ids in the cells below depending on which region you are in and which inference profiles are available.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have AWS credentials or AWS profile setup before running this cell\n",
    "region_name = 'us-east-1'\n",
    "bedrock_client = boto3.client('bedrock', region_name=region_name)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Inference Profiles\n",
    "The inference profiles feature comes with 2 additional APIs in the Bedrock client SDK:\n",
    "\n",
    "* `list_inference_profiles()` -> This API tells you all the models that are configured behind Inference Profiles for you.\n",
    "* `get_inference_profile(inferenceProfileIdentifier)` -> This API give you specific details of a certain Inference Profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.list_inference_profiles()['inferenceProfileSummaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that a Inference Profile can exist in 2 forms:\n",
    "\n",
    "**Foundation model in primary region**\n",
    "\n",
    "In this mode a Inference Profile is only configured for a model which exists in the primary region. For such model(s) a failover mechanism would exist allowing requests to be re-routed to secondary regions configured in Inference Profile. By default the request will go to primary region resources and only if the region is busy or you hit your quota limit, the request is routed to another region. In order to determine, which region the request should go to Amazon Bedrock intelligently checks in real-time which region has redundant capacity available. The on-demand principle still applies if none of the region has capacity to handle the request, then it will be throttled.\n",
    "\n",
    "In this mode, If the primary region doesn't have a certain model available, you will not be able to access it in a secondary region via cross-region inference feature.\n",
    "\n",
    "**Available via Inference Profiles**\n",
    "\n",
    "Select list of models are made available via Inference Profile, where Amazon Bedrock abstracts away the regional details and manages the hosting and inference routing automatically. Such foundation models then exist across the pre-defined region sets and the builder can build applications agnostic of setting up a region. This allows reliable throughput, access to leading foundation models as well as scalability in terms of throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.get_inference_profile(\n",
    "    inferenceProfileIdentifier='us.anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_inference_profile` API you can observe the `status` if a Inference Profile is `ACTIVE` or not and also which regions are configured for inference routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Inference Profiles\n",
    "An Inference Profile is used in the same way as a foundation model using the `modelId` or `arn` of the model. Inference profile also comes with it's own id and arn, where the difference is in the prefix. For the inference profile you can expect a regional prefix such as `us.` or `eu.` behind the model id for it to be recognized as an inference profile. Also in the `arn`, you can find the difference from `:<region>::foundation-model/<model-id>` to `:<region>::inference-profile/<region-set-prefix>.<model-id>`\n",
    "\n",
    "You can use both the `arn` and the `modelId` with `Converse` API whereas only the `modelId` with `InvokeModel` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converse API\n",
    "Amazon Bedrock now supports a unified messaging API for seamless application building experience. Read about all the models supported via this API [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n",
    "\n",
    "Let's send a request to both the foundation model as well as the Inference Profile to observe change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "system_prompt = \"You are an expert on AWS services and always provide correct and concise answers.\"\n",
    "input_message = \"Should I be storing documents in Amazon S3 or EFS for cost effective applications?\"\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-haiku-20240307-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-haiku-20240307-v1:0')\n",
    "\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=id,\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": input_message}]\n",
    "        }]\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response['output']['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that using the same API and only the change in model id you can expect similar behavior.\n",
    "\n",
    "It is also possible to use `inferenceProfileArn` with the `Converse` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceProfileArn = 'arn:aws:bedrock:us-east-1::inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inferenceProfileArn,\n",
    "    system=[{\"text\": system_prompt}],\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_message}]\n",
    "    }]\n",
    ")\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InvokeModel API\n",
    "Most of the generative AI applications in production are already built on top of `InvokeModel` API, even the third-party tools also use this API for using the models. The cross-region inference feature is also compatible with this API. Where the Converse API only supports select models, all models available in Amazon Bedrock can leverage InvokeModel API.\n",
    "\n",
    "Let's send a request via both the foundation model id as well as the inference profile id to observe change via this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": system_prompt,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{input_message}\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=id, accept=accept, contentType=contentType)\n",
    "    end = time()\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response_body['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain\n",
    "Below you can find integration on how to use the new Inference Profiles feature with one of the popular open-source frameworks [LangChain](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_aws langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/v0.2/docs/introduction/) with the help of integrations implements [`langchain-aws`](https://github.com/langchain-ai/langchain-aws) which provides support  `Converse` API via [`ChatBedrockConverse`](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/#beta-bedrock-converse-api). This allows you to use the latest models such as Anthropic Claude 3 Sonnet via this implementation. Below you can see an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Inference Profiles provide the ability to manage bursts and spiky traffic patterns across a variety of generative AI workloads and disparate request shapes.With this feature you can easily scale your workloads in production without the need of heavy-lifting, lengthy migrations and overhead of infrastructure management. Amazon Bedrock handles the routing securely, reliably and in a transparent manner while giving you the control you need.\n",
    "\n",
    "### Key considerations\n",
    "While building or migrating applications to use Inference Profiles, it is important to keep in mind the following:\n",
    "- **Latency** - If your application is latency sensitive, it is advised to properly test the use of Inference Profiles prior to fully relying on it since the routing to different regions could lead to higher latency numbers thus impacting your application behavior.\n",
    "- **Compliance** - Inference Profiles come with pre-defined region sets, if these sets contain a region where you can't operate or goes against your policy, then it is advised not to use Inference Profiles, instead utilize Foundation Models directly.\n",
    "- **Determinism** - If you need to have control over where your requests are (or will be) re-routed (other than primary region), it is better to consider just rely on Foundation Model directly. Also the Inference Profile exclusive models should be opted for carefully.\n",
    "- **Variety** - Inference Profiles do not provide access to mulitple different models from different regions, it either acts as a failover mechanism for models in primary region or provides Inference Profile exclusive models where the construct of a region is abstracted away. If your business demands to span across variety of foundation models from mulitple regions, it is wise to consider building your own architecture via VPC Peering/Transit Gateway or other architectural patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AGR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
